{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "from mish import Mish as mish\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, Conv1D, GlobalAveragePooling1D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recharger les données d'entraînement, de validation et test en fichier numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((451962, 32),\n",
       " (451962,),\n",
       " (153469, 32),\n",
       " (153469,),\n",
       " (153469,),\n",
       " (207485, 32),\n",
       " (207485,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recharger les données après avoir vidé la mémoire\n",
    "train_X = np.load('Data/data_MLP/train_X.npy')\n",
    "train_y = np.load('Data/data_MLP/train_y.npy')\n",
    "\n",
    "valid_X = np.load('Data/data_MLP/valid_X.npy')\n",
    "valid_y = np.load('Data/data_MLP/valid_y.npy')\n",
    "valid_id = np.load('Data/data_MLP/valid_id.npy')\n",
    "\n",
    "test_X = np.load('Data/data_MLP/test_X.npy')\n",
    "test_id = np.load('Data/data_MLP/test_id.npy')\n",
    "train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, valid_id.shape, test_X.shape, test_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder les labels entre 0 et 4 de sorte à matcher les prédictions des réseaux de neurones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5], dtype=uint8),\n",
       " array([0, 1, 2, 3, 4]),\n",
       " array([1, 2, 3, 4, 5], dtype=uint8),\n",
       " array([0, 1, 2, 3, 4]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_y)\n",
    "train_y_enc = encoder.transform(train_y)\n",
    "valid_y_enc = encoder.transform(valid_y)\n",
    "np.unique(train_y), np.unique(train_y_enc), np.unique(valid_y), np.unique(valid_y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 10,757\n",
      "Trainable params: 10,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "1758/1766 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.8455\n",
      "Epoch 00001: val_acc improved from -inf to 0.84786, saving model to my_model/model\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.4814 - acc: 0.8456 - val_loss: 0.3856 - val_acc: 0.8479\n",
      "Epoch 2/25\n",
      "1762/1766 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8815\n",
      "Epoch 00002: val_acc improved from 0.84786 to 0.87794, saving model to my_model/model\n",
      "1766/1766 [==============================] - 14s 8ms/step - loss: 0.3635 - acc: 0.8815 - val_loss: 0.3251 - val_acc: 0.8779\n",
      "Epoch 3/25\n",
      "1763/1766 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8936\n",
      "Epoch 00003: val_acc improved from 0.87794 to 0.88735, saving model to my_model/model\n",
      "1766/1766 [==============================] - 11s 6ms/step - loss: 0.3215 - acc: 0.8936 - val_loss: 0.3026 - val_acc: 0.8873\n",
      "Epoch 4/25\n",
      "1763/1766 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.8995\n",
      "Epoch 00004: val_acc improved from 0.88735 to 0.88804, saving model to my_model/model\n",
      "1766/1766 [==============================] - 13s 7ms/step - loss: 0.3007 - acc: 0.8995 - val_loss: 0.3064 - val_acc: 0.8880\n",
      "Epoch 5/25\n",
      "1765/1766 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9035\n",
      "Epoch 00005: val_acc improved from 0.88804 to 0.89241, saving model to my_model/model\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.2875 - acc: 0.9035 - val_loss: 0.2971 - val_acc: 0.8924\n",
      "Epoch 6/25\n",
      "1758/1766 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9065\n",
      "Epoch 00006: val_acc improved from 0.89241 to 0.89440, saving model to my_model/model\n",
      "1766/1766 [==============================] - 14s 8ms/step - loss: 0.2766 - acc: 0.9066 - val_loss: 0.2962 - val_acc: 0.8944\n",
      "Epoch 7/25\n",
      "1765/1766 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9098\n",
      "Epoch 00007: val_acc did not improve from 0.89440\n",
      "1766/1766 [==============================] - 13s 7ms/step - loss: 0.2675 - acc: 0.9098 - val_loss: 0.3036 - val_acc: 0.8935\n",
      "Epoch 8/25\n",
      "1760/1766 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9117\n",
      "Epoch 00008: val_acc improved from 0.89440 to 0.89601, saving model to my_model/model\n",
      "1766/1766 [==============================] - 8s 4ms/step - loss: 0.2608 - acc: 0.9118 - val_loss: 0.3056 - val_acc: 0.8960\n",
      "Epoch 9/25\n",
      "1761/1766 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9126\n",
      "Epoch 00009: val_acc improved from 0.89601 to 0.89649, saving model to my_model/model\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.2546 - acc: 0.9126 - val_loss: 0.2887 - val_acc: 0.8965\n",
      "Epoch 10/25\n",
      "1761/1766 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9150\n",
      "Epoch 00010: val_acc did not improve from 0.89649\n",
      "1766/1766 [==============================] - 13s 7ms/step - loss: 0.2476 - acc: 0.9150 - val_loss: 0.2988 - val_acc: 0.8946\n",
      "Epoch 11/25\n",
      "1766/1766 [==============================] - ETA: 0s - loss: 0.2416 - acc: 0.9171\n",
      "Epoch 00011: val_acc improved from 0.89649 to 0.89893, saving model to my_model/model\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.2416 - acc: 0.9171 - val_loss: 0.2973 - val_acc: 0.8989\n",
      "Epoch 12/25\n",
      "1760/1766 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9183\n",
      "Epoch 00012: val_acc did not improve from 0.89893\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.2362 - acc: 0.9183 - val_loss: 0.2918 - val_acc: 0.8981\n",
      "Epoch 13/25\n",
      "1760/1766 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9202\n",
      "Epoch 00013: val_acc improved from 0.89893 to 0.90030, saving model to my_model/model\n",
      "1766/1766 [==============================] - 10s 6ms/step - loss: 0.2304 - acc: 0.9202 - val_loss: 0.2918 - val_acc: 0.9003\n",
      "Epoch 14/25\n",
      "1761/1766 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9216\n",
      "Epoch 00014: val_acc did not improve from 0.90030\n",
      "1766/1766 [==============================] - 11s 6ms/step - loss: 0.2260 - acc: 0.9216 - val_loss: 0.3019 - val_acc: 0.8993\n",
      "Epoch 15/25\n",
      "1761/1766 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9234\n",
      "Epoch 00015: val_acc improved from 0.90030 to 0.90068, saving model to my_model/model\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.2202 - acc: 0.9234 - val_loss: 0.2957 - val_acc: 0.9007\n",
      "Epoch 16/25\n",
      "1762/1766 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9250\n",
      "Epoch 00016: val_acc did not improve from 0.90068\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.2166 - acc: 0.9250 - val_loss: 0.3125 - val_acc: 0.8989\n",
      "Epoch 17/25\n",
      "1759/1766 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9263\n",
      "Epoch 00017: val_acc improved from 0.90068 to 0.90086, saving model to my_model/model\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.2118 - acc: 0.9263 - val_loss: 0.2996 - val_acc: 0.9009\n",
      "Epoch 18/25\n",
      "1757/1766 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9272\n",
      "Epoch 00018: val_acc did not improve from 0.90086\n",
      "1766/1766 [==============================] - 12s 7ms/step - loss: 0.2078 - acc: 0.9272 - val_loss: 0.3044 - val_acc: 0.9001\n",
      "Epoch 19/25\n",
      "1760/1766 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9290\n",
      "Epoch 00019: val_acc did not improve from 0.90086\n",
      "1766/1766 [==============================] - 8s 4ms/step - loss: 0.2044 - acc: 0.9290 - val_loss: 0.3119 - val_acc: 0.9001\n",
      "Epoch 20/25\n",
      "1731/1766 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9304\n",
      "Epoch 00020: val_acc did not improve from 0.90086\n",
      "1766/1766 [==============================] - 5s 3ms/step - loss: 0.2002 - acc: 0.9304 - val_loss: 0.3194 - val_acc: 0.9006\n",
      "Epoch 21/25\n",
      "1752/1766 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9308\n",
      "Epoch 00021: val_acc did not improve from 0.90086\n",
      "1766/1766 [==============================] - 3s 2ms/step - loss: 0.1976 - acc: 0.9308 - val_loss: 0.3436 - val_acc: 0.8892\n",
      "Epoch 22/25\n",
      "1730/1766 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9320\n",
      "Epoch 00022: val_acc improved from 0.90086 to 0.90169, saving model to my_model/model\n",
      "1766/1766 [==============================] - 3s 2ms/step - loss: 0.1949 - acc: 0.9320 - val_loss: 0.3141 - val_acc: 0.9017\n",
      "Epoch 23/25\n",
      "1738/1766 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9333\n",
      "Epoch 00023: val_acc did not improve from 0.90169\n",
      "1766/1766 [==============================] - 3s 2ms/step - loss: 0.1912 - acc: 0.9333 - val_loss: 0.3429 - val_acc: 0.8942\n",
      "Epoch 24/25\n",
      "1757/1766 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9339\n",
      "Epoch 00024: val_acc did not improve from 0.90169\n",
      "1766/1766 [==============================] - 3s 2ms/step - loss: 0.1895 - acc: 0.9339 - val_loss: 0.3555 - val_acc: 0.8923\n",
      "Epoch 25/25\n",
      "1744/1766 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9351\n",
      "Epoch 00025: val_acc did not improve from 0.90169\n",
      "1766/1766 [==============================] - 3s 2ms/step - loss: 0.1862 - acc: 0.9351 - val_loss: 0.3124 - val_acc: 0.9013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd45038b950>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(32)))\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer,loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "# Définir un callback pour sauver les poids de votre modèle sur les meilleures époque c'est à dire les moments où il s'améliorera sur le jeu de validation\n",
    "Path('my_model').mkdir(exist_ok=True, parents=True)\n",
    "checkpointpath = os.path.join('my_model','model') # chemin où sauver le modèle\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
    "              checkpointpath,\n",
    "              verbose=1, # niveau de log\n",
    "              monitor='val_acc', # nom de la métrique à surveiller\n",
    "              save_best_only=True, # sauver uniquement le meilleur modèle\n",
    "              #shuffle=True,\n",
    "              save_weights_only=True)] # sauver uniquement les poids\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 25\n",
    "model.fit (train_X, train_y_enc, validation_data=(valid_X,valid_y_enc), batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)\n",
    "# loss: 0.1080 - acc: 0.9614 - val_loss: 0.4343 - val_acc: 0.8908 (100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd4804e1950>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restaurer les poids du modèle sur la meilleure époque d'entraînement\n",
    "model.load_weights(checkpointpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 1s 840us/step - loss: 0.3141 - acc: 0.9017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3140886425971985, 0.9016935229301453]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S'assurer que c'est bien le meilleur modèle sur les époques d'entraînement\n",
    "model.evaluate(valid_X,valid_y_enc,batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prédiction des classes sur le jeu de validation et évaluation en aggrégeant au niveau objet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153469, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Récupérer les probabilités prédites sur le jeu de validation\n",
    "valid_prob = model.predict(valid_X,batch_size=256)\n",
    "valid_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153469,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retourner la classe correspondant à la probabilité la plus haute\n",
    "valid_pred = np.argmax(valid_prob,axis=1) # axe 1 car ceci concerne chaque ligne\n",
    "valid_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Je réencode les prédictions entre 1 et 5\n",
    "valid_pred_enc = encoder.inverse_transform(valid_pred)\n",
    "np.unique(valid_pred_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggrégation au niveau objet\n",
    "out_pred = []\n",
    "unique_id = np.unique(valid_id)\n",
    "for ID in unique_id :\n",
    "    # Récupérer les prédictions des pixels appartenant au même objet\n",
    "    pred = valid_pred_enc[np.where(valid_id==ID)]\n",
    "    y_true = valid_y[np.where(valid_id==ID)]\n",
    "    # Prendre la valeur majoritaire des prédictions sur les pixels\n",
    "    out_pred.append([ np.bincount(y_true).argmax(), np.bincount(pred).argmax()]) #(Vérité terrain,Prédiction majoritaire)\n",
    "out_pred = np.vstack(out_pred)\n",
    "out_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8675284522916036"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score au niveau objet\n",
    "f1_score(out_pred[:,0],out_pred[:,1],average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prédire sur le jeu test et Préparer une soumission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207485, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Récupérer les probabilités prédites sur le jeu test\n",
    "test_prob = model.predict(test_X,batch_size=256)\n",
    "test_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207485,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retourner la classe correspondant à la probabilité la plus haute\n",
    "test_pred = np.argmax(test_prob,axis=1) # axe 1 car ceci concerne chaque ligne\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Je réencode les prédictions entre 1 et 5\n",
    "test_pred_enc = encoder.inverse_transform(test_pred)\n",
    "np.unique(test_pred_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggrégation au niveau objet\n",
    "agg_pred = []\n",
    "unique_id = np.unique(test_id)\n",
    "for ID in unique_id :\n",
    "    # Récupérer les prédictions des pixels appartenant au même objet\n",
    "    pred = test_pred_enc[np.where(test_id==ID)]\n",
    "    # Prendre la valeur majoritaire des prédictions sur les pixels\n",
    "    agg_pred.append([ ID, np.bincount(pred).argmax()]) #(ID,Prédiction majoritaire)\n",
    "agg_pred = np.vstack(agg_pred)\n",
    "agg_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Class\n",
       "0   4      5\n",
       "1   7      3\n",
       "2   8      5\n",
       "3   9      3\n",
       "4  10      3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'ID':agg_pred[:,0],'Class':agg_pred[:,1]})\n",
    "df.to_csv('Output/Soumission_GoFocus_MLP.csv',index=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8485908690152663"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 Score au niveau objet\n",
    "df_test = pd.read_csv('Data/Test_id_Label.csv') # Ce fichier vous sera fourni le 12 Novembre\n",
    "f1_score(df_test.Class,df.Class,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test 1: 0.8168138256780346 #Dense(1, activation=tf.nn.sigmoid) #50 epochs\n",
    "#test 2: 0.858408874794737 #commenter Dense(1, activation=tf.nn.sigmoid) #50 epochs\n",
    "#test 3: 0.8531057408753407 #commenter un dropout #50 epochs\n",
    "#test 4: 0.8505898715655911 #learning rate 0.01 #50 epochs\n",
    "#test 5: 0.8358556090157446 #learning rate 0.01 et 2 dropour #50 epochs\n",
    "#test 6: 0.6681578964596924 #sans shuffle drop(2)\n",
    "#test 7: 0.839245722036305 #sans shuffle drop(32)\n",
    "#test 8: 0.8508922745770693 #sans shuffle drop(128)\n",
    "#test 9: 0.8378991833114442 #avec shuffle drop(128)\n",
    "#test 10: 0.8025656496225863 #avec shuffle drop(128) et sigmoid\n",
    "#test 11: 0.8477687744222915 #sans sigmoid learning rate 0.001 sans shuffle\n",
    "#test 12: 0.8403317047002465 #avec shuffle\n",
    "#test 13: 0.8489819756056037 #1 drop avec shuffle\n",
    "#test 14: 0.8502387911808984 #3 couches dense \n",
    "#test 15: 0.8437704547495312 #sans shuffle 3 couche dense learning rate 0.01\n",
    "#test 16: 0.8445219779416981 #sans shuffle 3 couche dense learning rate 0.001\n",
    "#test 17: 0.8102395489221204 #sans shuffle 3 couche dense learning rate 0.001 avec fonction sigmoid\n",
    "#test 18: 0.010465116279069769 #sans shuffle 3 couche dense learning rate 0.01 avec fonction sigmoid\n",
    "#test 19: 0.8264949456297309 #sans shuffle 3 couche dense learning rate 0.01 \n",
    "#test 20: 0.815481059283146\n",
    "#test 21: 0.8336245042433846 \n",
    "#test 22: 0.8477613639540746 \n",
    "#test 23: 0.8535262044634899 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
