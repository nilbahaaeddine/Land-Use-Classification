{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, Conv1D, GlobalAveragePooling1D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compte rendu de classification\n",
    "def cpt_mal_classes(y_test_func, result_func):\n",
    "    nb_func = 0\n",
    "    for i in range(len(y_test_func)):\n",
    "        if y_test_func[i] != result_func[i]:\n",
    "            nb_func += 1\n",
    "    print (f'Taille des données {len(y_test_func)} mal classés {nb_func}\\n')\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('Output').mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recharger les données d'entraînement, de validation et test en fichier numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recharger les données après avoir vidé la mémoire\n",
    "train_X = np.load('Data/train_valid_test/train_X.npy')\n",
    "train_y = np.load('Data/train_valid_test/train_y.npy')\n",
    "\n",
    "valid_X = np.load('Data/train_valid_test/valid_X.npy')\n",
    "valid_y = np.load('Data/train_valid_test/valid_y.npy')\n",
    "valid_id = np.load('Data/train_valid_test/valid_id.npy')\n",
    "\n",
    "test_X = np.load('Data/train_valid_test/test_X.npy')\n",
    "test_id = np.load('Data/train_valid_test/test_id.npy')\n",
    "\n",
    "train_X = train_X[:,2,2,:,:]\n",
    "valid_X = valid_X[:,2,2,:,:]\n",
    "test_X = test_X[:,2,2,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder les labels entre 0 et 4 de sorte à matcher les prédictions des réseaux de neurones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_y)\n",
    "train_y_enc = encoder.transform(train_y)\n",
    "valid_y_enc = encoder.transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=5, activation='relu', input_shape=(8, 4)))\n",
    "model.add(Conv1D(64, activation='relu', kernel_size=3))\n",
    "model.add(Conv1D(128, activation='relu', kernel_size=1))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = tf.keras.optimizers.Adam(0.01)\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 25\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
    "              'Model/model',\n",
    "              verbose=1, # niveau de log\n",
    "              monitor='val_accuracy', # nom de la métrique à surveiller\n",
    "              save_best_only=True, # sauver uniquement le meilleur modèle\n",
    "              save_weights_only=True)] # sauver uniquement les poids\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_X, train_y_enc, validation_data=(valid_X,valid_y_enc), batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)\n",
    "# loss: 0.2481 - accuracy: 0.9138 - val_loss: 0.2786 - val_accuracy: 0.9017 (10)\n",
    "# loss: 0.0982 - acc: 0.9655 - val_loss: 0.4826 - val_acc: 0.8791 (100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Output/Best')\n",
    "\n",
    "model_loaded1 = model.load_weights('Model/model')\n",
    "model_loaded2 = keras.models.load_model('Output/Best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(valid_X, valid_y_enc, batch_size=256)\n",
    "\n",
    "print(f'Loss : {score[0]:.2f}')\n",
    "print(f'Accuracy : {score[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prédiction des classes sur le jeu de validation et évaluation en aggrégeant au niveau objet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les probabilités prédites sur le jeu de validation\n",
    "valid_prob = model.predict(valid_X,batch_size=256)\n",
    "valid_prob.shape\n",
    "\n",
    "# Retourner la classe correspondant à la probabilité la plus haute\n",
    "valid_pred = np.argmax(valid_prob,axis=1) # axe 1 car ceci concerne chaque ligne\n",
    "valid_pred.shape\n",
    "\n",
    "# Je réencode les prédictions entre 1 et 5\n",
    "valid_pred_enc = encoder.inverse_transform(valid_pred)\n",
    "np.unique(valid_pred_enc)\n",
    "\n",
    "# Aggrégation au niveau objet\n",
    "out_pred = []\n",
    "unique_id = np.unique(valid_id)\n",
    "for ID in unique_id :\n",
    "    # Récupérer les prédictions des pixels appartenant au même objet\n",
    "    pred = valid_pred_enc[np.where(valid_id==ID)]\n",
    "    y_true = valid_y[np.where(valid_id==ID)]\n",
    "    # Prendre la valeur majoritaire des prédictions sur les pixels\n",
    "    out_pred.append([ np.bincount(y_true).argmax(), np.bincount(pred).argmax()]) #(Vérité terrain,Prédiction majoritaire)\n",
    "out_pred = np.vstack(out_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt_mal_classes(out_pred[:,0], out_pred[:,1])\n",
    "print(f'F1 score : {f1_score(out_pred[:,0],out_pred[:,1],average=\"weighted\"):.2f}\\n')\n",
    "print(f'Accuracy : {accuracy_score(out_pred[:,1], out_pred[:,0]):.2f}\\n')\n",
    "print(f'Matrice de confusion :\\n{confusion_matrix(out_pred[:,0], out_pred[:,1])}\\n')\n",
    "print(f'Classification report :\\n{classification_report(out_pred[:,0], out_pred[:,1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(out_pred[:,0], out_pred[:,1]), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prédire sur le jeu test et Préparer une soumission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les probabilités prédites sur le jeu test\n",
    "test_prob = model.predict(test_X,batch_size=256)\n",
    "test_prob.shape\n",
    "\n",
    "# Retourner la classe correspondant à la probabilité la plus haute\n",
    "test_pred = np.argmax(test_prob,axis=1) # axe 1 car ceci concerne chaque ligne\n",
    "test_pred.shape\n",
    "\n",
    "# Je réencode les prédictions entre 1 et 5\n",
    "test_pred_enc = encoder.inverse_transform(test_pred)\n",
    "np.unique(test_pred_enc)\n",
    "\n",
    "# Aggrégation au niveau objet\n",
    "agg_pred = []\n",
    "unique_id = np.unique(test_id)\n",
    "for ID in unique_id :\n",
    "    # Récupérer les prédictions des pixels appartenant au même objet\n",
    "    pred = test_pred_enc[np.where(test_id==ID)]\n",
    "    # Prendre la valeur majoritaire des prédictions sur les pixels\n",
    "    agg_pred.append([ ID, np.bincount(pred).argmax()]) #(ID,Prédiction majoritaire)\n",
    "agg_pred = np.vstack(agg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'ID':agg_pred[:,0],'Class':agg_pred[:,1]})\n",
    "df_test = pd.read_csv('Data/Test_id_Label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt_mal_classes(df_test.Class, df.Class)\n",
    "print(f'F1 score : {f1_score(df_test.Class,df.Class,average=\"weighted\"):.2f}\\n')\n",
    "print(f'Accuracy : {accuracy_score(df.Class, df_test.Class):.2f}\\n')\n",
    "print(f'Matrice de confusion :\\n{confusion_matrix(df_test.Class, df.Class)}\\n')\n",
    "print(f'Classification report :\\n{classification_report(df_test.Class, df.Class)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(df_test.Class, df.Class), None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
